Turn sounds into bits or numbers (features)
The first step in working with audio files is how to turn an audio wave into numbers so that you can feed this into your machine learning algorithm.

You can then build a spectrogram of the audio file:

Even though this is a great way to visually see an audio wave, you need to still need to use the Mel Frequency Cepstral Coefficient in order to extract the features from your audio files.
MFCCs is the best way to capture audio features and similar to how the human auditory system processes sound and frequencies.
The Mel scale will basically transform linear frequencies to a log scale



Divide test and train data
Once I had my data ready, the next logical step was to then divided the data into test and train with a 20% split in order to start preparing dataframe for our model.

Label encoding
Since you are using strings for your emotions, it is important that you use label encoding to transform these emotions into numbers; otherwise, your model just won’t run!
#Encode emotion labels into numbers


Scaling data
This step is very important as it will prevent your model to fit the volume level of your audio recordings.

Try Random Forest, only 35% accuracy

Try 1 Dimension Convolutional Neural Network



┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ conv1d_4 (Conv1D)               │ (None, 40, 16)         │            96 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_5 (Conv1D)               │ (None, 40, 32)         │         2,592 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_6 (Conv1D)               │ (None, 40, 64)         │        10,304 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_7 (Conv1D)               │ (None, 40, 128)        │        41,088 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_2 (Dropout)             │ (None, 40, 128)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten_1 (Flatten)             │ (None, 5120)           │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_3 (Dense)                 │ (None, 128)            │       655,488 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_3 (Dropout)             │ (None, 128)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_4 (Dense)                 │ (None, 64)             │         8,256 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_5 (Dense)                 │ (None, 8)              │           520 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 718,344 (2.74 MB)
 Trainable params: 718,344 (2.74 MB)
 Non-trainable params: 0 (0.00 B)





# Compile the model with the desired loss function, optimizer, and metric to optimize
CNN_model.compile(loss = 'categorical_crossentropy',
                  optimizer = 'Adam',
                  metrics = ['accuracy'])

CNN - Model Accuracy PNG
Pre-training accuracy: 66.6667%

the model went through 25 epochs and maxes out at 10–12 epochs with a 67% accuracy


Training Accuracy:  0.9982638955116272
Testing Accuracy:  0.6666666865348816


build a confusion matrix in order to visualize what audio files are being misclassified:

Next Steps To Improve Model
There are a number of next logical steps that I could do to improve the model and keep testing:

Balance data set and see if you reduce certain emotions if the model improves.
Polish my notebooks and change structure to
Add gender to the target variables. Increase target variables from 8 to 16 and see if you include gender how a new model would perform.
Keep working on the set up of the 1D CNN by adding layers, max pooling and other variables that could improve model.
Change the Adam default optimizer in the 1D CNN of 10% to 1% to see if the learning rate improves and does not plateau at 12 epochs.
Record my own audio files and see how the model performs on other datasets.

























